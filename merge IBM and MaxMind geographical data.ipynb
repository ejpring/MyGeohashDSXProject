{
    "metadata": {
        "language_info": {
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "pygments_lexer": "ipython3", 
            "file_extension": ".py", 
            "name": "python", 
            "version": "3.5.2", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python"
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 1.6 (Unsupported)", 
            "name": "python3"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## merge IBM and MaxMind geographical data\n\n-----\n\nThe code in this notebook merges IBM internal network data and Internet geography files for use in IBM Streaming Analytics, such as the NetflowViewer demonstration and cyber-security applications:\n\n* Data for the IBM internal network comes from an Excel file provided by Mark Harvey (Mark_Harvey@uk.ibm.com). The file contains separate spreadsheets for IBM marketing regions US, EMEA, and AP. These spreadsheets list the IP subnet addresses IBM has assigned to its offices in each region, along with country, city, and street address (but not state/province/territory or latitude/longitude). All of these subnets are within the class A subnet 9.xxx.xxx.xxx assigned to IBM.\n\n* The Internet geography data comes from CSV files provided by [MaxMind, Inc.](https://www.maxmind.com/en/home) as [GeoLite2 data](https://dev.maxmind.com/geoip/geoip2/geolite2/). This notebook downloads the 'GeoLite2' data, which MaxMind offers free of charge. The files list IP subnets in the Internet, along with country, state/province/territory, city, latitude, and longitude. MaxMind updates this data once a month, and this notebook downloads the data into a directory whose name includes the date of the files.\n\nThis notebook geocodes the IBM data with state/province/territory and latitude/longitude data from Google using its Geocoding API service. Google limits this service to 2,500 requests per day, which is sufficient for several runs of the cell below that geocodes a list of IBM locations. \n\nFinally, this notebook merges the geocoded IBM data into the MaxMind CSV files. It also generates a separate CSV file containing a [geohash code](https://en.wikipedia.org/wiki/Geohash) for each city's latitude/longitude. All of the resulting CSV files are packed into a ZIP file for transfer to Streaming Analytics projects:\n\n* [GeoLite2-City-Blocks-IPv4.csv](merged/GeoLite2-City-Blocks-IPv4.csv)\n* [GeoLite2-City-Blocks-IPv6.csv](merged/GeoLite2-City-Blocks-IPv6.csv)\n* [GeoLite2-City-Locations-en.csv](merged/GeoLite2-City-Locations-en.csv)\n* [GeoLite2-City-Geohashes-en.csv](merged/GeoLite2-City-Geohashes-en.csv)\n\nTo use this notebook, you will need to provide these things:\n\n* an Excel file named 'report_IGA_Global_Q1_2016.xlsx' containing IBM internal network data\n\n* a Google Maps geocoding API key for a valid Google account\n\nThere are detailed instructions these steps in the cells below."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "-----\nRun this cell once to install additional function packages:"
        }, 
        {
            "metadata": {}, 
            "execution_count": 54, 
            "cell_type": "code", 
            "source": "!pip install --user googlemaps\n!pip install --user geohash2", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Collecting googlemaps\nRequirement already satisfied (use --upgrade to upgrade): requests<3.0,>=2.11.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sa73-1acf9232f65bd2-cf1c60ef4a00/.local/lib/python3.5/site-packages (from googlemaps)\nRequirement already satisfied (use --upgrade to upgrade): urllib3<1.23,>=1.21.1 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sa73-1acf9232f65bd2-cf1c60ef4a00/.local/lib/python3.5/site-packages (from requests<3.0,>=2.11.1->googlemaps)\nRequirement already satisfied (use --upgrade to upgrade): chardet<3.1.0,>=3.0.2 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sa73-1acf9232f65bd2-cf1c60ef4a00/.local/lib/python3.5/site-packages (from requests<3.0,>=2.11.1->googlemaps)\nRequirement already satisfied (use --upgrade to upgrade): certifi>=2017.4.17 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sa73-1acf9232f65bd2-cf1c60ef4a00/.local/lib/python3.5/site-packages (from requests<3.0,>=2.11.1->googlemaps)\nRequirement already satisfied (use --upgrade to upgrade): idna<2.7,>=2.5 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sa73-1acf9232f65bd2-cf1c60ef4a00/.local/lib/python3.5/site-packages (from requests<3.0,>=2.11.1->googlemaps)\nInstalling collected packages: googlemaps\nSuccessfully installed googlemaps-2.5.1\nCollecting geohash2\nRequirement already satisfied (use --upgrade to upgrade): docutils>=0.3 in /gpfs/global_fs01/sym_shared/YPProdSpark/user/sa73-1acf9232f65bd2-cf1c60ef4a00/.local/lib/python3.5/site-packages (from geohash2)\nInstalling collected packages: geohash2\nSuccessfully installed geohash2-1.1\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "-----\nRun this cell to set up the notebook's runtime environment:"
        }, 
        {
            "metadata": {}, 
            "execution_count": 55, 
            "cell_type": "code", 
            "source": "import os\nimport math\nimport pprint\nimport shutil\nimport zipfile\nimport types\n\n# load functions for maniulating matrixes \nimport pandas as pd\npd.set_option('max_rows', 15)\n\n# load functions for reading and writing byte streams\nfrom io import BytesIO\n\n# load functions for reading URLs\nfrom urllib.request import urlopen\n\n# load functions for reading and writing Cloud Object Storage\nimport ibm_boto3\nfrom ibm_botocore.client import Config\n\n# load functions for the Google Maps geocoding API\nimport googlemaps\n\n# load functions for converting latitude/longitude coordinates into geohash codes\nimport geohash2\n\n# create a local directory for staging merged CSV files\nos.makedirs('merged', exist_ok=True)", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "-----\nThen, provide the Excel file containing IBM internal network data available to this notebook. To do this, copy the file to the notebook's Cloud Object Storage bucket and create an HTTP client for reading and writing files:\n\n* open the 'Files' panel by clicking the 'Data' icon in the upper-right corner of this DSX project,\n\n* drag Excel file 'report_IGA_Global_Q1_2016.xlsx' from your laptip to the 'drop' area in the 'Files' panel,\n\n* position the cursor at the top of the next cell and click 'Insert to code -> Insert Credentials' in the 'Files' panel,\n\n* make sure the name of the inserted variable is 'credentials', and\n\n* run the cell to set the credentials in the variable."
        }, 
        {
            "metadata": {}, 
            "execution_count": 56, 
            "cell_type": "code", 
            "source": "# The code was removed by DSX for sharing.", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "-----\nRun the following cell to read IBM network addresses and locations from the Excel file's spreadsheets into 'pandas' frames:"
        }, 
        {
            "metadata": {}, 
            "execution_count": 57, 
            "cell_type": "code", 
            "source": "print('running ...')\n\n# create a Cloud Object Store HTTP client with the bucket's credentials\ncosClient = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n    config=Config(signature_version='oauth'),\n    endpoint_url=credentials['ENDPOINT'])\n\n# get a byte stream for reading the Excel file from the bucket\nexcelStream = cosClient.get_object(Bucket=credentials['BUCKET'], Key=credentials['FILE'])['Body']\n\n# add an iterator method to the stream object so pandas will accept it as a file-like object\ndef __iter__(self): return 0\nif not hasattr(excelStream, \"__iter__\"): excelStream.__iter__ = types.MethodType(__iter__, excelStream) \n\n# prepare to read the byte stream from Cloud Object Store as an Excel file\nexcelFile = pd.ExcelFile(excelStream)\n\n# concatenate the data in each spreadsheet of the Excel file into a single pandas DataFrame\nibmData = pd.concat( map(lambda sheet: pd.read_excel(excelFile, sheet, header=0), excelFile.sheet_names) )\n\n# correct some misencoded city names in the 'report_IGA_Global_Q1_2016.xlsx' spreadsheets\nibmCorrections = { 'S?O Paulo': 'Sao Paulo', 'Quer?Taro': 'Queretaro' }\nfor name in ibmCorrections:\n    ibmData.loc[ibmData['# City']==name, '# City'] = ibmCorrections[name]\n\n# create a frame with the country, city, and street address of each IBM internal IP subnet\n    ibmNetworks = ibmData[['# Network Container', '# Country', '# City', '# Street']].dropna()\nibmNetworks = ibmNetworks[ibmNetworks['# Network Container'].str.match(\"[0-9./]+\")]\nibmNetworks = ibmNetworks.drop_duplicates('# Network Container',keep='first')\nibmNetworks.columns = ['network', 'country_name', 'city_name', 'street_address']    \n                       \n# create a frame with the country, city, and street address of each IBM location, and empty columns for more geographical data\nibmLocations = ibmNetworks.drop('network',axis=1).drop_duplicates(['country_name', 'city_name', 'street_address'])\nibmLocations['geoname_id'] = range(len(ibmLocations))\nibmLocations['country_iso_code'] = None\nibmLocations['subdivision_1_iso_code'] = None\nibmLocations['subdivision_1_name'] = None\nibmLocations['subdivision_2_iso_code'] = None\nibmLocations['subdivision_2_name'] = None\nibmLocations['postal_code'] = None\nibmLocations['latitude'] = None\nibmLocations['longitude'] = None\n\nprint('... done')", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "running ...\n... done\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "-----\nRun the next cell to download and unpack Internet network and location data from MaxMind into 'pandas' frames:"
        }, 
        {
            "metadata": {}, 
            "execution_count": 58, 
            "cell_type": "code", 
            "source": "maxmindURL = 'http://geolite.maxmind.com/download/geoip/database/GeoLite2-City-CSV.zip'\n\nprint('running ...')\n\nwith urlopen(maxmindURL) as response:\n    with zipfile.ZipFile(BytesIO(response.read())) as file:\n        file.extractall()\n\nmaxmindDirectory = sorted( [ f for f in os.listdir() if os.path.isdir(f) and f.startswith('GeoLite2-City-CSV') ] )[-1]\nmaxmindLocations = pd.read_csv(maxmindDirectory + '/GeoLite2-City-Locations-en.csv', header=0)\nmaxmindNetworks = pd.read_csv(maxmindDirectory + '/GeoLite2-City-Blocks-IPv4.csv', header=0)\n\nprint('... done')", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "running ...\n... done\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "-----\nNext, get a Google geocoding API key for a valid Google account:\n\n* In a browser, go to [Google](https://www.google.com/) and sign into an existing account or create a new account.\n\n* Go to the [Google Geocoding Service](https://developers.google.com/maps/documentation/javascript/geocoding) page and follow the instructions to create a project and enable the geocoding API.\n\n* Go to [Google Geocoding Service 'Get API Key'](https://developers.google.com/maps/documentation/geocoding/get-api-key), click on 'Get a Key', and then click the 'copy' button.\n\n* paste the copied key into the next cell as the value of the 'googlemapsKey' variable \n\n* run the next cell to set key in the variable\n\nNote that this service is limited to 2,500 requests per day. That is sufficient for several runs of the cell below that geocodes a list of IBM cities. "
        }, 
        {
            "metadata": {}, 
            "execution_count": 59, 
            "cell_type": "code", 
            "source": "# The code was removed by DSX for sharing.", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Run the next cell to fill the empty geography columns for IBM locations with data from the Google geocoding service. Note that Google limits geocoding API requests to 2,500 per day. There are about 450 IBM locations, so you can run this cell several times in the same day before reaching the limit. After the limit is reached, the Google 'geocode()' function will return an error code for the remainder of the day."
        }, 
        {
            "metadata": {
                "scrolled": false
            }, 
            "execution_count": 60, 
            "cell_type": "code", 
            "source": "def convertAddressToGeocode(client, address):\n    result = client.geocode(address)\n    ###########print('>>>>>>>>>>>client.geocode(' + address + ') returned:')\n    #######pprint.pprint(result,width=150)\n    if result is None: return None\n    if len(result)<1: return None\n    if 'address_components' not in result[0]: return None\n    geocode = dict( [ (i['types'][0],{'long_name':i['long_name'],'short_name': i['short_name']}) for i in result[0]['address_components'] ] )\n    geocode['latitude'] = result[0]['geometry']['location']['lat']\n    geocode['longitude'] = result[0]['geometry']['location']['lng']\n    ########print('>>>>>>>>>>>convertAddressToGeocode(' + address + ') returned:')\n    ##########pprint.pprint(geocode,width=150)\n    return geocode\n\nK = 500\n\ndef geocodeIBMLocationRow(googlemapsClient, row):\n    if row['geoname_id']>K: return row\n    address = \"IBM, \" + row['street_address'] + ', ' + row['city_name'] + ', ' + row['country_name']\n    geocode = convertAddressToGeocode(googlemapsClient, address)\n    if geocode is None:\n        address = \"IBM, \" + row['city_name'] + ', ' + row['country_name']\n        geocode = convertAddressToGeocode(googlemapsClient, address)\n    if geocode is None: \n        print('address not found: ' + address)\n        return row\n    try: \n        if 'country' in geocode: \n            row['country_iso_code'] = geocode['country']['short_name']\n        if 'administrative_area_level_1' in geocode: \n            row['subdivision_1_iso_code'] = geocode['administrative_area_level_1']['short_name']\n            row['subdivision_1_name'] = geocode['administrative_area_level_1']['long_name']\n        if 'administrative_area_level_2' in geocode: \n            row['subdivision_2_iso_code'] = geocode['administrative_area_level_2']['short_name']\n            row['subdivision_2_name'] = geocode['administrative_area_level_2']['long_name']\n        if 'postal_code' in geocode: \n            row['postal_code'] = geocode['postal_code']['long_name']\n        if 'latitude' in geocode: \n            row['latitude'] = geocode['latitude']\n            row['longitude'] = geocode['longitude']\n    except KeyError as e: \n        print(str(e) + 'not found for address ' + address) \n    return row\n\nprint('running ...')\n\n# create an HTTP client for using the Google Maps geocoding API\ngooglemapsClient = googlemaps.Client(key=googlemapsKey)\n\n# geocode each IBM location\nibmLocations = ibmLocations.apply(lambda row: geocodeIBMLocationRow(googlemapsClient, row), axis=1)\n\nprint('... done')", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "running ...\naddress not found: IBM, Acheson, Canada\naddress not found: IBM, Florenceville, Canada\naddress not found: IBM, West Des Moines, United States\naddress not found: IBM, Coerdoba, Argentina\naddress not found: IBM, Horsham, Great Britain\naddress not found: IBM, Porto Salvo, Portugal\naddress not found: IBM, Aachen, Germany\naddress not found: IBM, Chemnitz, Germany\naddress not found: IBM, Flensburg, Germany\naddress not found: IBM, Walldorf, Germany\naddress not found: IBM, Olsztyn, Poland\naddress not found: IBM, Gdansk, Poland\naddress not found: IBM, Cagliari, Italy\naddress not found: IBM, Catania, Italy\naddress not found: IBM, Napoli, Italy\naddress not found: IBM, Vaesteraes, Sweden\naddress not found: IBM, Riga, Latvia\naddress not found: IBM, Lugano, Switzerland\naddress not found: IBM, Aubagne, France\naddress not found: IBM, Luanda, Angola\naddress not found: IBM, Accra, Ghana\naddress not found: IBM, Palma De Mallorca, Spain\naddress not found: IBM, Shijiazhuang, China\naddress not found: IBM, Kunming, China\naddress not found: IBM, Zhengzhou, China\naddress not found: IBM, Guiyang, China\naddress not found: IBM, Jinan, China\n... done\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "-----\nRun the next cell to merge the MaxMind and IBM locations. The merged data will be written into a CSV file in the 'merged' directory named 'GeoLite2-City-Locations-en.csv'."
        }, 
        {
            "metadata": {}, 
            "execution_count": 61, 
            "cell_type": "code", 
            "source": "print('running ...')\n\n# create a frame of IBM locations with 'IBM' in the city name\nibmLocationsWithRelabeledCity = ibmLocations\nibmLocationsWithRelabeledCity['city_name'] = 'IBM ' + ibmLocations['city_name']\n\n# merge the MaxMind and IBM location frames and store the result as a CSV file\nmergedLocations = pd.concat([maxmindLocations,ibmLocationsWithRelabeledCity[ list( set(maxmindLocations.columns) & set(ibmLocationsWithRelabeledCity.columns) ) ]])\nmergedLocations.to_csv('merged/GeoLite2-City-Locations-en.csv', index=False, float_format='%.9g', columns=maxmindLocations.columns)\n\nprint('... done')", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "running ...\n... done\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "-----\nRun the next cell to merge the MaxMind and IBM networks. The merged data will be written into a CSV file in the 'merged' directory named 'GeoLite2-City-Blocks-IPv4.csv'."
        }, 
        {
            "metadata": {}, 
            "execution_count": 62, 
            "cell_type": "code", 
            "source": "print('running ...')\n\n# create a frame of IBM networks with 'IBM' in the city name\nibmNetworksWithRelabeledCity = ibmNetworks\nibmNetworksWithRelabeledCity['city_name'] = 'IBM ' + ibmNetworksWithRelabeledCity['city_name']\n\n# create a frame of IBM locations indexed by country, city, and street address\nibmLocationsIndexed = ibmLocationsWithRelabeledCity.set_index(['country_name','city_name','street_address'])\n\n# add country, city, and street address for each network in the IBM networks frame\nibmNetworksWithLocation = ibmNetworksWithRelabeledCity.join(ibmLocationsIndexed, on=['country_name','city_name','street_address'])\n\n# merge the MaxMind and IBM network frames and store the result in a CSV file\nmergedNetworks = pd.concat([maxmindNetworks,ibmNetworksWithLocation[ list( set(maxmindNetworks.columns) & set(ibmNetworksWithLocation.columns) ) ]])\nmergedNetworks.to_csv('merged/GeoLite2-City-Blocks-IPv4.csv', index=False, float_format='%.9g', columns=maxmindNetworks.columns)\n                                                        \nprint('... done')", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "running ...\n... done\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "-----\nRun the next cell to calculate [geohash codes](https://en.wikipedia.org/wiki/Geohash) for the latitude/longitude coordinates of merged MaxMind and IBM locations. The geohashes, coordinates, and location data will be written into a CSV file in the 'merged' directory named 'GeoLite2-City-Geohashes-en.csv'."
        }, 
        {
            "metadata": {}, 
            "execution_count": 63, 
            "cell_type": "code", 
            "source": "print('running ...')\n\n# create a frame of locations indexed by ID number\nmergedLocationsIndexed = mergedLocations.set_index('geoname_id')\n\n# create a frame of geographical coordinates, that is, ID number, latitude, and longitude\nmergedCoordinates = mergedNetworks[['geoname_id','latitude','longitude']].drop_duplicates()\n\n# merge location and coordinate data and calculate geohash for each location's coordinates\nmergedGeohashes = mergedCoordinates.join(mergedLocationsIndexed, on='geoname_id')\nmergedGeohashes['geohash'] = mergedGeohashes.apply(lambda row: geohash2.encode(row['latitude'],row['longitude'],precision=6),axis=1)\n\n# store the result in a CSV file\ncolumns = ['geohash','latitude','longitude','geoname_id','country_iso_code','country_name','subdivision_1_iso_code','subdivision_1_name','subdivision_2_iso_code','subdivision_2_name','city_name']\nmergedGeohashes.to_csv('merged/GeoLite2-City-Geohashes-en.csv', index=False, float_format='%.9g', columns=columns)\n                                                        \nprint('... done')", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "running ...\n... done\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "-----\nFinally, pack all of the merged CSV files into a ZIP package and copy it to the notebook's bucket in Cloud Object Storage ...."
        }, 
        {
            "metadata": {}, 
            "execution_count": 64, 
            "cell_type": "code", 
            "source": "resultPackage = 'mergedIBMandInternetGeographyData.zip'\n\nprint('running ...')\n\n# add the MaxMind IPv6 network file to the ZIP package\nshutil.copy(maxmindDirectory + '/GeoLite2-City-Blocks-IPv6.csv', 'merged')\n\n# pack all result files into a ZIP package\nwith zipfile.ZipFile(resultPackage, 'w', compression=zipfile.ZIP_DEFLATED) as zipFile:\n    for file in os.listdir('merged'):\n        zipFile.write('merged/'+file, file)\n\n# write the ZIP file to the notebook's bucket in Cloud Object Storage\ncosClient.upload_file(Filename=resultPackage, Bucket=credentials['BUCKET'], Key=resultPackage)\n\nprint('... done')", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "running ...\n... done\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "-----\nTo download the ZIP package containing the results of merging IBM and Internet geography data, do this:\n\n* In a browser, go to this notebook's project page\n\n* open the 'Files' panel by clicking the 'Find and Add Data' icon in the upper-right corner of the project page,\n\n* check the box next to 'mergedIBMandInternetGeographyData.zip'\n\n* select 'Download' from the pop-up menu in the 'Files' panel"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/\\/"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "-----\nOptionally, run this last cell to clean up the notebook's runtime environment. This is really not necessary."
        }, 
        {
            "metadata": {}, 
            "execution_count": 65, 
            "cell_type": "code", 
            "source": "#!rm -rf *\n#!pip uninstall -y googlemaps geohash2 ", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "execution_count": 66, 
            "cell_type": "code", 
            "source": "ls -al", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "total 115872\r\ndrwx------  4 sa73-1acf9232f65bd2-cf1c60ef4a00 users     4096 Feb 11 15:08 \u001b[0m\u001b[01;34m.\u001b[0m/\r\ndrwx------ 11 sa73-1acf9232f65bd2-cf1c60ef4a00 users     4096 Feb 11 11:48 \u001b[01;34m..\u001b[0m/\r\ndrwx------  2 sa73-1acf9232f65bd2-cf1c60ef4a00 users     4096 Feb 11 15:02 \u001b[01;34mGeoLite2-City-CSV_20180206\u001b[0m/\r\ndrwx------  2 sa73-1acf9232f65bd2-cf1c60ef4a00 users     4096 Feb 11 15:08 \u001b[01;34mmerged\u001b[0m/\r\n-rw-------  1 sa73-1acf9232f65bd2-cf1c60ef4a00 users 39549657 Feb 11 15:08 mergedIBMandInternetGeographyData.zip\r\n"
                }
            ]
        }, 
        {
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 2
}